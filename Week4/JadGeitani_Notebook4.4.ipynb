{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeKJzdjhwAj-"
      },
      "source": [
        "\n",
        "\n",
        "# AAI612: Deep Learning & its Applications\n",
        "\n",
        "*Notebook 4.3: Graded Assignment: Mini Project I*\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jgeitani/AAI612_Geitani/blob/main/Week4/JadGeitani_Notebook4.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8MCpsSFwAkC"
      },
      "source": [
        "# Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX5kHPgpwAkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMDILOiwAkE"
      },
      "source": [
        "In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzin1qU5wAkF"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWA0pKmPwAkF"
      },
      "source": [
        "In this exercise, you will train a model to recognize fresh and rotten fruits. Download the dataset from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification). The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym2m3tfcwAkG"
      },
      "source": [
        "![image.png](attachment:4c8c02c9-0cbe-4048-8d01-cdd5e3cf3fe6.png)<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sriramr/fruits-fresh-and-rotten-for-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg1WI7t41Z5C",
        "outputId": "2a33cd5e-3f81-42b3-b3a4-67df124f59dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sriramr/fruits-fresh-and-rotten-for-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.58G/3.58G [00:35<00:00, 108MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQTHbBsb8VUT",
        "outputId": "826d6d4a-e2cc-4fed-84d4-ba99e3b64838"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Check GPU details\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTlAJ_S9CBdV",
        "outputId": "64fc523a-9d50-4737-b9f8-dfe300b38818"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Fri Feb 14 10:41:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0             31W /   70W |     230MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTj53eIuwAkG"
      },
      "source": [
        "## Load ImageNet Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUwrZJpwAkG"
      },
      "source": [
        "Start with a model pretrained on `ImageNet`. Load the model with the correct weights, set an input shape, and choose to remove the last layers of the model. Remember that images have three dimensions: a height, and width, and a number of channels. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. This cannot be changed or the assessment will fail. If you need a reference for setting up the pretrained model, please take a look at [Notebook 4.2](https://github.com/harmanani/AAI612/blob/main/Week4/Notebook%204.2.ipynb) where we implemented transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zYU-dGk1wAkH"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "from tensorflow import keras\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "\n",
        "base_model = keras.applications.VGG16(\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otTHEmqOwAkI"
      },
      "source": [
        "## Freeze Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCCUX7evwAkJ"
      },
      "source": [
        "Next, we suggest freezing the base model. This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AjEu3N_fwAkJ"
      },
      "outputs": [],
      "source": [
        "# Freeze base model\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPG3zkHZwAkK"
      },
      "source": [
        "## Add Layers to Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DthNqYNwAkK"
      },
      "source": [
        "Now it's time to add layers to the pretrained model. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit.  You may add more layers than specified below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LMUMKhqbwAkK"
      },
      "outputs": [],
      "source": [
        "# Create inputs with correct shape\n",
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Add pooling layer or flatten layer\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add final dense layer\n",
        "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
        "\n",
        "# Combine inputs and outputs to create model\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "VzBjg2VvwAkL",
        "outputId": "a84f4a1d-ca4a-439b-8fde-95eebd139e10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │           \u001b[38;5;34m3,078\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,717,766\u001b[0m (56.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,717,766</span> (56.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,078\u001b[0m (12.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> (12.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YJHNZOLwAkL"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vK5mdaQwAkL"
      },
      "source": [
        "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bQ-v06HywAkM"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=False) , metrics = [keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAdM4jpAwAkM"
      },
      "source": [
        "## Augment the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYZxj0SjwAkM"
      },
      "source": [
        "If you'd like, try to augment the data to improve the dataset. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-4daZNDlwAkM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(samplewise_center=True,\n",
        "        rotation_range=10,\n",
        "        zoom_range = 0.1,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwRE2QgNwAkN"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRgfbJIQwAkN"
      },
      "source": [
        "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIfJcFrEwAkN",
        "outputId": "932229dc-8cce-48c7-cf00-133bbdd9ace1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10901 images belonging to 6 classes.\n",
            "Found 2698 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "# load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory('/root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1/dataset/train',\n",
        "                                       target_size=(224, 224),\n",
        "                                       color_mode='rgb',\n",
        "                                       class_mode=\"categorical\")\n",
        "# load and iterate validation dataset\n",
        "valid_it = datagen.flow_from_directory('/root/.cache/kagglehub/datasets/sriramr/fruits-fresh-and-rotten-for-classification/versions/1/dataset/test',\n",
        "                                      target_size=(224, 224),\n",
        "                                      color_mode='rgb',\n",
        "                                      class_mode=\"categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsMZuk8iwAkO"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Q9pnZiwAkO"
      },
      "source": [
        "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting your desired number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Q_LYa2wAkO",
        "outputId": "0937d96c-4f1d-49ce-bab8-3a34f610903f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 660ms/step - categorical_accuracy: 0.6274 - loss: 1.8455 - val_categorical_accuracy: 0.9275 - val_loss: 0.2023\n",
            "Epoch 2/20\n",
            "\u001b[1m  1/340\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 156ms/step - categorical_accuracy: 0.9375 - loss: 0.1698"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 119ms/step - categorical_accuracy: 0.9375 - loss: 0.1698 - val_categorical_accuracy: 0.9457 - val_loss: 0.1509\n",
            "Epoch 3/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 770ms/step - categorical_accuracy: 0.9478 - loss: 0.1547 - val_categorical_accuracy: 0.9632 - val_loss: 0.0945\n",
            "Epoch 4/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 0.9375 - loss: 0.1219 - val_categorical_accuracy: 0.9632 - val_loss: 0.0982\n",
            "Epoch 5/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 604ms/step - categorical_accuracy: 0.9634 - loss: 0.1008 - val_categorical_accuracy: 0.9743 - val_loss: 0.0737\n",
            "Epoch 6/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - categorical_accuracy: 0.9688 - loss: 0.2370 - val_categorical_accuracy: 0.9758 - val_loss: 0.0689\n",
            "Epoch 7/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 604ms/step - categorical_accuracy: 0.9787 - loss: 0.0617 - val_categorical_accuracy: 0.9721 - val_loss: 0.0782\n",
            "Epoch 8/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 0.0107 - val_categorical_accuracy: 0.9762 - val_loss: 0.0662\n",
            "Epoch 9/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 602ms/step - categorical_accuracy: 0.9779 - loss: 0.0586 - val_categorical_accuracy: 0.9773 - val_loss: 0.0566\n",
            "Epoch 10/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 0.9688 - loss: 0.0940 - val_categorical_accuracy: 0.9736 - val_loss: 0.0694\n",
            "Epoch 11/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 774ms/step - categorical_accuracy: 0.9827 - loss: 0.0447 - val_categorical_accuracy: 0.9773 - val_loss: 0.0707\n",
            "Epoch 12/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 120ms/step - categorical_accuracy: 0.9688 - loss: 0.0369 - val_categorical_accuracy: 0.9650 - val_loss: 0.0923\n",
            "Epoch 13/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 609ms/step - categorical_accuracy: 0.9857 - loss: 0.0410 - val_categorical_accuracy: 0.9847 - val_loss: 0.0412\n",
            "Epoch 14/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 0.9688 - loss: 0.0617 - val_categorical_accuracy: 0.9833 - val_loss: 0.0514\n",
            "Epoch 15/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 773ms/step - categorical_accuracy: 0.9858 - loss: 0.0378 - val_categorical_accuracy: 0.9874 - val_loss: 0.0318\n",
            "Epoch 16/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 0.0012 - val_categorical_accuracy: 0.9851 - val_loss: 0.0448\n",
            "Epoch 17/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 773ms/step - categorical_accuracy: 0.9848 - loss: 0.0383 - val_categorical_accuracy: 0.9855 - val_loss: 0.0464\n",
            "Epoch 18/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 0.0050 - val_categorical_accuracy: 0.9855 - val_loss: 0.0405\n",
            "Epoch 19/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 716ms/step - categorical_accuracy: 0.9860 - loss: 0.0322 - val_categorical_accuracy: 0.9885 - val_loss: 0.0352\n",
            "Epoch 20/20\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 116ms/step - categorical_accuracy: 1.0000 - loss: 0.0046 - val_categorical_accuracy: 0.9892 - val_loss: 0.0296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e8b1a5f9650>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model.fit(train_it,\n",
        "          validation_data=valid_it,\n",
        "          steps_per_epoch=int(train_it.samples/train_it.batch_size),\n",
        "          validation_steps=int(valid_it.samples/valid_it.batch_size),\n",
        "          epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5taMo3u0wAkO"
      },
      "source": [
        "## Unfreeze Model for Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0yVfhM0wAkP"
      },
      "source": [
        "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LklqAd5TwAkP"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile the model with a low learning rate\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001), loss = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        " , metrics = [keras.metrics.CategoricalAccuracy()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NukVhnmIwAkQ",
        "outputId": "64cf6b13-5286-4f79-fc81-639e4e4543f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 795ms/step - categorical_accuracy: 0.9752 - loss: 0.0828 - val_categorical_accuracy: 0.9754 - val_loss: 0.0699\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/340\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 455ms/step - categorical_accuracy: 1.0000 - loss: 0.0249"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 122ms/step - categorical_accuracy: 1.0000 - loss: 0.0249 - val_categorical_accuracy: 0.9903 - val_loss: 0.0263\n",
            "Epoch 3/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 768ms/step - categorical_accuracy: 0.9959 - loss: 0.0123 - val_categorical_accuracy: 0.9948 - val_loss: 0.0146\n",
            "Epoch 4/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 0.0082 - val_categorical_accuracy: 0.9978 - val_loss: 0.0053\n",
            "Epoch 5/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 886ms/step - categorical_accuracy: 0.9972 - loss: 0.0105 - val_categorical_accuracy: 0.9981 - val_loss: 0.0039\n",
            "Epoch 6/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 1.1856e-04 - val_categorical_accuracy: 0.9985 - val_loss: 0.0028\n",
            "Epoch 7/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 771ms/step - categorical_accuracy: 0.9994 - loss: 0.0027 - val_categorical_accuracy: 0.9959 - val_loss: 0.0094\n",
            "Epoch 8/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - categorical_accuracy: 1.0000 - loss: 7.2165e-04 - val_categorical_accuracy: 0.9978 - val_loss: 0.0065\n",
            "Epoch 9/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 769ms/step - categorical_accuracy: 0.9992 - loss: 0.0041 - val_categorical_accuracy: 0.9996 - val_loss: 0.0026\n",
            "Epoch 10/10\n",
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 120ms/step - categorical_accuracy: 1.0000 - loss: 1.1176e-08 - val_categorical_accuracy: 0.9985 - val_loss: 0.0026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e8b1a125d10>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model.fit(train_it,\n",
        "          validation_data=valid_it,\n",
        "          steps_per_epoch=int(train_it.samples/train_it.batch_size),\n",
        "          validation_steps=int(valid_it.samples/valid_it.batch_size),\n",
        "          epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxdTS4IVwAkQ"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT-wrg-swAkR"
      },
      "source": [
        "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6OibZGGwAkR"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a multi-class classification problem, so we used the CategoricalCrossentropy and set the from_logits to False and the output layer uses a softmax activation."
      ],
      "metadata": {
        "id": "IkaN5rZdcKvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model evaluates performance based on how often the predicted class matches the actual class."
      ],
      "metadata": {
        "id": "XE15NbMncr-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base model (VGG16 in this case) was initially frozen then later on was unfrozen. While freezing the model, we trained only the custom top layers, then we fine-tuned the entire model by unfreezing the base and using a lower learning rate."
      ],
      "metadata": {
        "id": "XByg6b_xdA87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now from the obtained results we can see that the model performed exceptionally well during the frozen and also the unfrozen phase. In the frozen phase, there were early overfitting signs where the training accuracy reached 100% by epoch 8, while validation accuracy improved at a slower pace to become stable around 98-99%. However, the validation loss continued to decrease showing that the model generalized well."
      ],
      "metadata": {
        "id": "wSSmOYqhdlPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then in the unfrozen phase, the validation accuracy reached 99.96% at epoch 9, and the validation loss kept decreasing until it reached 0.0026 at epoch 10. We can see that in the unfrozen phase there is no overfitting since the difference between training accuracy and validation accuracy is almost negligible."
      ],
      "metadata": {
        "id": "61e_SfgdeNx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results, we can see that his model has a high validation accuracy and extremely low validation loss, which means that the model generalizes well to unseen data."
      ],
      "metadata": {
        "id": "2fjtwi9Ce9Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve these results we used a pre-trained model (VGG16),  fine-tuning by unfreezing the model and setting a low learning rate, we also used the appropriate loss and metrics which are CategoricalCrossentropy and CattegoricalAccuracy in this case. Reaching a validation accuracy of 99% and a validation loss of 0.0026 is considered great and shows that the model performs well with new data."
      ],
      "metadata": {
        "id": "1sGDvQQlfa4_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}